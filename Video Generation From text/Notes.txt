Video Generation From Text

We tackle this problem by training a conditional generative model to extract 
both static and dynamic information from text.
this is manifested in a hybrid framework, employing a variational autoencoder (VAE) 
and a generative adversial network (GAN). the static features, called "gist", are used
to sketch text-conditioned background color and object layout structure.

we breakdown the generation task into two components. First, a conditional VAE
model is used to generate the "gist" of the video from the input text, where the gist is an 
image that gives the background color and object layout of the desired video.

the gist-generation step extracts static and "universal" features from the text, while the video
generot extracts the dynamic and "detailed" information from the text.

THREE MODELS

GIST GENERATOR

In a short video clip, the background is usually static with
only small motion changes. The gist generator uses a CVAE
to produce the static background from the text 

The output
of this CVAE network is called “gist”, which is then one of
the inputs to the video generator.

VIDEO GENERATOR

The video is generated by three entangled neural networks,
in a GAN framework, adopting the ideas of (Vondrick, Pirsi-
avash, and Torralba 2016). The GAN framework is trained by
having a generator and a discriminator compete in a minimax
game (Goodfellow et al. 2014). The generator synthesizes
fake samples to confuse the discriminator, while the discrimi-
nator aims to accurately distinguish synthetic and real sam-
ples.

As mentioned, conditional GANs have been previously
used to construct images from text (Reed et al. 2016). Be-
cause this work needs to condition on both the gist and text,
it is unfortunately complicated to construct gist-text-video
triplets in a similar manner. Instead, first a motion filter is
computed based on the text t and applied to the gist, further
described in Section 3.3. This step forces the model to use
the text information to generate plausible motion; simply
concatenating the feature sets allows the text information to
be given minimal importance on motion generation. 

TEXT2FILTER

Simply concatenating the gist and text encoding empirically
resulted in an overly reliant usage of either gist or text in-
formation. Tuning the length and relative strength of the
features is challenging in a complex framework. Instead, a
more-robust and effective way to utilize the text information
is to construct the motion-generating filter weights based on
the text information, which is denoted by Text2Filter.

This paper proposes a framework for generating video from
text using a hybrid VAE-GAN framework. To the best of
our knowledge, this work proposes the first successful frame-
work for video generation from text. The intermediate gist-
generation step greatly helps enforce the static background
of video from input text. 

DATASET USED

YOUTUBE VIDEOS WITH SOME PREPROCESSING

For each keyword, we first collected
a set of videos together with their title, description, duration
and tags from YouTube. The dataset was then cleaned by
outlier-removal techniques. Specifically, the method of (Berg,
Berg, and Shih 2010) was used to get the 10 most frequent
tags for the set of video. The quality of the selected tags is
further guaranteed by matching them to the words in exist-
ing categories in ImageNet (Deng et al. 2009) and Action-
Bank (Sadanand and Corso 2012). These two datasets help
ensure that the selected tags have visually detectable objects
and actions. Only videos with at least three of the selected
tags were included. Other requirements include (i) the du-
ration of the video should be within the range of 10 to 400
seconds, (ii) the title and description should be in English,
and (iii) the title should have more than four meaningful
words after removing numbers and stop words.

Clean videos from the Kinetics Human Action Video
Dataset (Kinetics) (Kay et al . 2017) are additionally used with
the steps described above to further expand the dataset.